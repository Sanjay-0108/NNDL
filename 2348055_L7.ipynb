{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAVdgwMSLA7o",
    "outputId": "dad8f243-af83-4831-e581-14a7c2c051ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Uni\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the news website\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract all paragraphs from the webpage\n",
    "paragraphs = soup.find_all('p')\n",
    "text_content = ' '.join([para.get_text() for para in paragraphs])\n",
    "\n",
    "# Print the extracted text content\n",
    "print(text_content[:1000])  # Preview first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPSkGe6OLEct",
    "outputId": "7f99a9e2-fbed-44d4-c61c-45b1ae63bc13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Credit', 'Cards', 'Loans', 'Banking', 'Mortgages', 'Insurance', 'Credit', 'Monitoring', 'Personal', 'Finance', 'Small', 'Business', 'Taxes', 'Help', 'for', 'Low', 'Credit', 'Scores', 'Investing', 'SELECT']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK tokenizers\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text content\n",
    "tokens = word_tokenize(text_content)\n",
    "\n",
    "# Print some tokens for preview\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYHPE3ZBLPGK",
    "outputId": "6c8e0cc8-8b63-444d-e664-02afcb3bc1f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training: 7490\n",
      "Number of sentences in validation: 833\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('conll2002')\n",
    "\n",
    "# Load the dataset (Spanish version in this example)\n",
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "# Prepare sentences and labels\n",
    "def prepare_data(sents):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sent in sents:\n",
    "        words = [word for word, pos, tag in sent]\n",
    "        tags = [tag for word, pos, tag in sent]\n",
    "        sentences.append(words)\n",
    "        labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = prepare_data(train_sents)\n",
    "test_sentences, test_labels = prepare_data(test_sents)\n",
    "\n",
    "# Create a word and tag dictionary\n",
    "words = list(set(word.lower() for sent in train_sentences for word in sent))\n",
    "tags = list(set(tag for sent in train_labels for tag in sent))\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1  # Unknown words\n",
    "word2idx[\"PAD\"] = 0  # Padding\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "# Convert words and tags into integer indices\n",
    "max_len = 75  # Max sentence length\n",
    "X_train = [[word2idx.get(w.lower(), word2idx[\"UNK\"]) for w in s] for s in train_sentences]\n",
    "X_test = [[word2idx.get(w.lower(), word2idx[\"UNK\"]) for w in s] for s in test_sentences]\n",
    "\n",
    "# Pad sequences\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "\n",
    "# Convert labels to categorical data and pad\n",
    "y_train = [[tag2idx[tag] for tag in label] for label in train_labels]\n",
    "y_test = [[tag2idx[tag] for tag in label] for label in test_labels]\n",
    "\n",
    "y_train = pad_sequences(y_train, maxlen=max_len, padding='post', value=tag2idx[\"O\"])\n",
    "y_test = pad_sequences(y_test, maxlen=max_len, padding='post', value=tag2idx[\"O\"])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = [to_categorical(i, num_classes=len(tags)) for i in y_train]\n",
    "y_test = [to_categorical(i, num_classes=len(tags)) for i in y_test]\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Number of sentences in training: {len(X_train)}\")\n",
    "print(f\"Number of sentences in validation: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "tvF6DhRNLZIc",
    "outputId": "932da3d1-9bf3-43ad-cf74-cf9167f0eb76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 2s/step - accuracy: 0.9394 - loss: 0.8727 - val_accuracy: 0.9604 - val_loss: 0.3438\n",
      "Epoch 2/3\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 2s/step - accuracy: 0.9623 - loss: 0.3024 - val_accuracy: 0.9705 - val_loss: 0.2444\n",
      "Epoch 3/3\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 2s/step - accuracy: 0.9736 - loss: 0.2063 - val_accuracy: 0.9749 - val_loss: 0.2059\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Dropout\n",
    "\n",
    "# Ensure TensorFlow is in eager execution mode\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = len(word2idx)  # Vocabulary size\n",
    "output_dim = 64  # Embedding output size\n",
    "n_tags = len(tag2idx)  # Number of unique tags\n",
    "max_len = 75  # Max sentence length\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer (no input_length)\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True))\n",
    "\n",
    "# Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))\n",
    "\n",
    "# TimeDistributed layer to output for each token (ensure compatibility)\n",
    "model.add(TimeDistributed(Dense(n_tags, activation='softmax')))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=3, validation_data=(X_val, np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ2GxTi6LqfK",
    "outputId": "41098647-82d8-49f9-e9d5-c92546ac5e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 948ms/step - accuracy: 0.9705 - loss: 0.2326\n",
      "Test Accuracy: 0.9710129499435425\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734ms/step\n",
      "Test Sentence: la coruña , 23 may ( efecom ) . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Predicted entities: ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, np.array(y_test))\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict the Named Entities in a test sentence\n",
    "i = 0  # Choose a sentence index\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "\n",
    "print(\"Test Sentence:\", ' '.join([list(word2idx.keys())[list(word2idx.values()).index(idx)] for idx in X_test[i]]))\n",
    "print(\"Predicted entities:\", [idx2tag[idx] for idx in p[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pi3kLxxNXwa"
   },
   "source": [
    "### **Model Training Output:**\n",
    "\n",
    "\n",
    "- **48/48**: This means that 48 batches of data were processed during training.\n",
    "- **59s 1s/step**: The entire epoch took 59 seconds, and each batch took about 1 second to process.\n",
    "- **accuracy: 0.9709**: The model achieved an accuracy of 97.09% during training.\n",
    "- **loss: 0.2333**: The loss value is 0.2333, which indicates how well the model fits the data. Lower loss typically means better performance.\n",
    "\n",
    "\n",
    "- **Test Accuracy**: The accuracy on the test set is 97.11%, which suggests that the model is performing very well on unseen data.\n",
    "\n",
    "### **Test Sentence and Predicted Entities:**\n",
    "\n",
    "\n",
    "\n",
    "- **Test Sentence**: The model was tested on the sentence: `\"la coruña , 23 may ( efecom ) .\"`. The `PAD` tokens are padding to ensure a fixed length sequence for the model, and they are ignored.\n",
    "- **Predicted Entities**:\n",
    "  - `O`: \"Outside\" – no named entity.\n",
    "  - `B-LOC`: \"Beginning of a location entity\" – `\"la coruña\"` is identified as a location (the \"B-LOC\" tag indicates the beginning of this entity).\n",
    "  - `B-ORG`: \"Beginning of an organization entity\" – `\"efecom\"` is identified as an organization.\n",
    "\n",
    "The model successfully identified `\"la coruña\"` as a location (`B-LOC`) and `\"efecom\"` as an organization (`B-ORG`). All other tokens in the sentence were marked as outside any named entity (`O`).\n",
    "\n",
    "### Overall Interpretation:\n",
    "The model has learned to recognize named entities in text well, as reflected by the high accuracy (97.1%) both during training and testing. The predictions for the test sentence indicate that the model correctly identified entities like locations (LOC) and organizations (ORG).**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
